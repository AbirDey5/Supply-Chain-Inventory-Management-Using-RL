{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e3b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"f:/app files/ucrt64/bin/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supply Chain & Inventory Management using Reinforcement Learning (Lab Project)\n",
    "\n",
    "This is an inventory control simulation where an RL agent\n",
    "learns how much to order each day to minimize total cost.\n",
    "\n",
    "Features:\n",
    "- Environment with inventory, demand, optional lead time pipeline\n",
    "- Q-Learning agent with discrete states/actions\n",
    "- Baselines: Random policy, (s, S) order-up-to policy\n",
    "- Training + evaluation + plots\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    days_per_episode: int = 60\n",
    "    train_episodes: int = 10000  # Increased to allow for finer state-space learning\n",
    "    eval_episodes: int = 200\n",
    "\n",
    "    # inventory settings\n",
    "    max_inventory: int = 200\n",
    "    max_order: int = 100         # Slightly increased\n",
    "    order_step: int = 10         # Smaller steps (was 20) for finer control\n",
    "    init_inventory: int = 60\n",
    "\n",
    "    # demand settings\n",
    "    demand_min: int = 5\n",
    "    demand_max: int = 25\n",
    "\n",
    "    # lead time (0 means immediate arrival)\n",
    "    lead_time: int = 2\n",
    "\n",
    "    # cost settings\n",
    "    holding_cost: float = 0.5        # per unit per day\n",
    "    stockout_cost: float = 30.0      # HIGHER (was 12.0) to force RL to prioritize fulfillment\n",
    "    order_fixed_cost: float = 5.0    # Discourages tiny orders every single day\n",
    "    order_unit_cost: float = 0.1     # per unit ordered\n",
    "\n",
    "    # q-learning settings\n",
    "    gamma: float = 0.95\n",
    "    alpha: float = 0.1\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.01\n",
    "    epsilon_decay: float = 0.9995    # Slower decay for better exploration\n",
    "\n",
    "    # discretization (state bins) - SMALLER bins provide higher resolution\n",
    "    inv_bin_size: int = 5            # Was 10\n",
    "    pipeline_bin_size: int = 10      # Was 10\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Environment\n",
    "# -----------------------------\n",
    "class InventoryEnv:\n",
    "    \"\"\"\n",
    "    State includes:\n",
    "      - on_hand inventory\n",
    "      - pipeline inventory (sum of outstanding orders arriving in future)\n",
    "    Action:\n",
    "      - order quantity (discrete: 0, step, 2*step, ... max_order)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.rng = random.Random(cfg.seed)\n",
    "        self.actions = list(range(0, cfg.max_order + 1, cfg.order_step))\n",
    "\n",
    "        self.on_hand = 0\n",
    "        self.day = 0\n",
    "        self.pipeline = deque([0] * cfg.lead_time)  # quantities arriving over next days\n",
    "\n",
    "        # logs (for plotting in a single episode)\n",
    "        self.log_on_hand = []\n",
    "        self.log_demand = []\n",
    "        self.log_order = []\n",
    "        self.log_cost = []\n",
    "        self.log_pipeline = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.on_hand = self.cfg.init_inventory\n",
    "        self.day = 0\n",
    "        self.pipeline = deque([0] * self.cfg.lead_time)\n",
    "\n",
    "        self.log_on_hand.clear()\n",
    "        self.log_demand.clear()\n",
    "        self.log_order.clear()\n",
    "        self.log_cost.clear()\n",
    "        self.log_pipeline.clear()\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action_idx: int):\n",
    "        \"\"\"\n",
    "        One day transition:\n",
    "        1) receive arriving order (if lead time > 0)\n",
    "        2) place a new order (goes into pipeline)\n",
    "        3) demand occurs, fulfill from on_hand\n",
    "        4) compute reward = - total_cost\n",
    "        \"\"\"\n",
    "        assert 0 <= action_idx < len(self.actions)\n",
    "        order_qty = self.actions[action_idx]\n",
    "\n",
    "        # 1) receive arrivals\n",
    "        if self.cfg.lead_time > 0:\n",
    "            arrived = self.pipeline.popleft()\n",
    "            self.on_hand = min(self.cfg.max_inventory, self.on_hand + arrived)\n",
    "        else:\n",
    "            arrived = 0  # not used\n",
    "\n",
    "        # 2) place order -> enters pipeline or arrives immediately\n",
    "        if self.cfg.lead_time > 0:\n",
    "            self.pipeline.append(order_qty)\n",
    "        else:\n",
    "            self.on_hand = min(self.cfg.max_inventory, self.on_hand + order_qty)\n",
    "\n",
    "        # 3) demand\n",
    "        demand = self.rng.randint(self.cfg.demand_min, self.cfg.demand_max)\n",
    "        sold = min(self.on_hand, demand)\n",
    "        unmet = demand - sold\n",
    "        self.on_hand -= sold\n",
    "\n",
    "        # 4) costs\n",
    "        holding = self.cfg.holding_cost * self.on_hand\n",
    "        stockout = self.cfg.stockout_cost * unmet\n",
    "        order_fixed = self.cfg.order_fixed_cost if order_qty > 0 else 0.0\n",
    "        order_var = self.cfg.order_unit_cost * order_qty\n",
    "        total_cost = holding + stockout + order_fixed + order_var\n",
    "\n",
    "        reward = -total_cost\n",
    "\n",
    "        # logging\n",
    "        self.log_on_hand.append(self.on_hand)\n",
    "        self.log_demand.append(demand)\n",
    "        self.log_order.append(order_qty)\n",
    "        self.log_cost.append(total_cost)\n",
    "        self.log_pipeline.append(sum(self.pipeline) if self.cfg.lead_time > 0 else 0)\n",
    "\n",
    "        # done?\n",
    "        self.day += 1\n",
    "        done = (self.day >= self.cfg.days_per_episode)\n",
    "\n",
    "        next_state = self._get_state()\n",
    "        info = {\n",
    "            \"holding_cost\": holding,\n",
    "            \"stockout_cost\": stockout,\n",
    "            \"order_cost\": order_fixed + order_var,\n",
    "            \"unmet\": unmet,\n",
    "            \"demand\": demand\n",
    "        }\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Discretize inventory + pipeline into bins for tabular Q-learning.\"\"\"\n",
    "        inv_bin = min(self.cfg.max_inventory // self.cfg.inv_bin_size,\n",
    "                      self.on_hand // self.cfg.inv_bin_size)\n",
    "\n",
    "        pipe_sum = sum(self.pipeline) if self.cfg.lead_time > 0 else 0\n",
    "        pipe_max = self.cfg.max_order * max(1, self.cfg.lead_time)\n",
    "        pipe_bin = min(pipe_max // self.cfg.pipeline_bin_size,\n",
    "                       pipe_sum // self.cfg.pipeline_bin_size)\n",
    "\n",
    "        return (int(inv_bin), int(pipe_bin))\n",
    "\n",
    "    def state_space_size(self):\n",
    "        inv_bins = (self.cfg.max_inventory // self.cfg.inv_bin_size) + 1\n",
    "        pipe_max = (self.cfg.max_order * max(1, self.cfg.lead_time))\n",
    "        pipe_bins = (pipe_max // self.cfg.pipeline_bin_size) + 1\n",
    "        return inv_bins, pipe_bins\n",
    "\n",
    "    def action_space_size(self):\n",
    "        return len(self.actions)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Q-Learning Agent\n",
    "# -----------------------------\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env: InventoryEnv, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        inv_bins, pipe_bins = env.state_space_size()\n",
    "        n_actions = env.action_space_size()\n",
    "\n",
    "        self.Q = np.zeros((inv_bins, pipe_bins, n_actions), dtype=np.float32)\n",
    "        self.epsilon = cfg.epsilon_start\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def choose_action(self, state, greedy=False):\n",
    "        inv_bin, pipe_bin = state\n",
    "\n",
    "        if (not greedy) and (random.random() < self.epsilon):\n",
    "            return random.randrange(self.n_actions)\n",
    "\n",
    "        qvals = self.Q[inv_bin, pipe_bin]\n",
    "        # tie-breaking randomly\n",
    "        best = np.max(qvals)\n",
    "        best_actions = np.where(qvals == best)[0]\n",
    "        return int(np.random.choice(best_actions))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        inv, pipe = state\n",
    "        n_inv, n_pipe = next_state\n",
    "\n",
    "        best_next = np.max(self.Q[n_inv, n_pipe])\n",
    "        # Bellman Equation: $Q(s,a) = Q(s,a) + \\alpha [R + \\gamma \\max Q(s',a') - Q(s,a)]$\n",
    "        target = reward if done else (reward + self.cfg.gamma * best_next)\n",
    "\n",
    "        self.Q[inv, pipe, action] += self.cfg.alpha * (target - self.Q[inv, pipe, action])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.cfg.epsilon_end, self.epsilon * self.cfg.epsilon_decay)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Baseline Policies\n",
    "# -----------------------------\n",
    "def policy_random(env: InventoryEnv):\n",
    "    return random.randrange(env.action_space_size())\n",
    "\n",
    "def policy_order_up_to(env: InventoryEnv, target_level=80):\n",
    "    \"\"\"\n",
    "    Simple heuristic:\n",
    "      Order enough so (on_hand + pipeline_sum) reaches target_level\n",
    "    \"\"\"\n",
    "    current = env.on_hand\n",
    "    pipe = sum(env.pipeline) if env.cfg.lead_time > 0 else 0\n",
    "    need = max(0, target_level - (current + pipe))\n",
    "\n",
    "    # round to nearest available action step\n",
    "    step = env.cfg.order_step\n",
    "    qty = int(math.ceil(need / step) * step)\n",
    "    qty = min(env.cfg.max_order, qty)\n",
    "\n",
    "    # map qty to action index\n",
    "    return env.actions.index(qty)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Training / Evaluation\n",
    "# -----------------------------\n",
    "def run_episode(env: InventoryEnv, agent: QLearningAgent = None, policy_fn=None, greedy=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    total_cost = 0.0\n",
    "    total_unmet = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent is not None:\n",
    "            a = agent.choose_action(state, greedy=greedy)\n",
    "        else:\n",
    "            a = policy_fn(env)\n",
    "\n",
    "        next_state, reward, done, info = env.step(a)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_cost += (-reward)\n",
    "        total_unmet += info[\"unmet\"]\n",
    "\n",
    "        if agent is not None and (not greedy):\n",
    "            agent.update(state, a, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return {\n",
    "        \"reward\": total_reward,\n",
    "        \"cost\": total_cost,\n",
    "        \"unmet\": total_unmet\n",
    "    }\n",
    "\n",
    "def train_qlearning(cfg: Config):\n",
    "    env = InventoryEnv(cfg)\n",
    "    agent = QLearningAgent(env, cfg)\n",
    "\n",
    "    history = {\n",
    "        \"episode_cost\": [],\n",
    "        \"episode_unmet\": [],\n",
    "        \"epsilon\": []\n",
    "    }\n",
    "\n",
    "    for ep in range(cfg.train_episodes):\n",
    "        stats = run_episode(env, agent=agent, greedy=False)\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        history[\"episode_cost\"].append(stats[\"cost\"])\n",
    "        history[\"episode_unmet\"].append(stats[\"unmet\"])\n",
    "        history[\"epsilon\"].append(agent.epsilon)\n",
    "\n",
    "        if (ep + 1) % 1000 == 0:\n",
    "            print(f\"  Episode {ep+1}/{cfg.train_episodes} | Avg Cost: {np.mean(history['episode_cost'][-100:]):.2f}\")\n",
    "\n",
    "    return env, agent, history\n",
    "\n",
    "def evaluate(env: InventoryEnv, agent: QLearningAgent, cfg: Config):\n",
    "    # Evaluate Q-learning (greedy)\n",
    "    rl_costs, rl_unmet = [], []\n",
    "    for _ in range(cfg.eval_episodes):\n",
    "        stats = run_episode(env, agent=agent, greedy=True)\n",
    "        rl_costs.append(stats[\"cost\"])\n",
    "        rl_unmet.append(stats[\"unmet\"])\n",
    "\n",
    "    # Evaluate baselines\n",
    "    rnd_costs, rnd_unmet = [], []\n",
    "    for _ in range(cfg.eval_episodes):\n",
    "        stats = run_episode(env, agent=None, policy_fn=policy_random)\n",
    "        rnd_costs.append(stats[\"cost\"])\n",
    "        rnd_unmet.append(stats[\"unmet\"])\n",
    "\n",
    "    heu_costs, heu_unmet = [], []\n",
    "    for _ in range(cfg.eval_episodes):\n",
    "        stats = run_episode(env, agent=None, policy_fn=lambda e: policy_order_up_to(e, target_level=80))\n",
    "        heu_costs.append(stats[\"cost\"])\n",
    "        heu_unmet.append(stats[\"unmet\"])\n",
    "\n",
    "    results = {\n",
    "        \"RL\": (np.mean(rl_costs), np.mean(rl_unmet)),\n",
    "        \"Random\": (np.mean(rnd_costs), np.mean(rnd_unmet)),\n",
    "        \"OrderUpTo\": (np.mean(heu_costs), np.mean(heu_unmet)),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# ... (Plotting functions remain the same) ...\n",
    "\n",
    "def plot_training(history):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history[\"episode_cost\"])\n",
    "    ax.set_title(\"Training: Episode Cost\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_one_episode(env: InventoryEnv, agent: QLearningAgent):\n",
    "    run_episode(env, agent=agent, greedy=True)\n",
    "    days = list(range(1, len(env.log_on_hand) + 1))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(days, env.log_on_hand, label=\"On-hand Inventory\")\n",
    "    ax.plot(days, env.log_pipeline, label=\"Pipeline (Sum)\")\n",
    "    ax.set_title(\"Inventory & Pipeline Over Time (RL Episode)\")\n",
    "    ax.set_xlabel(\"Day\")\n",
    "    ax.set_ylabel(\"Units\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    cfg = Config()\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    print(\"Training Q-Learning agent...\")\n",
    "    env, agent, history = train_qlearning(cfg)\n",
    "\n",
    "    print(\"\\nEvaluating policies...\")\n",
    "    results = evaluate(env, agent, cfg)\n",
    "\n",
    "    print(\"\\nAverage results over\", cfg.eval_episodes, \"episodes:\")\n",
    "    print(f\"{'Policy':12s} | {'Avg Cost':12s} | {'Avg Unmet':10s}\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, (avg_cost, avg_unmet) in results.items():\n",
    "        print(f\"  {name:10s} | {avg_cost:12.2f} | {avg_unmet:10.2f}\")\n",
    "\n",
    "    plot_training(history)\n",
    "    plot_one_episode(env, agent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
